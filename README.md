# local_ai_chat

## example

```shell
ASSISTANT: ' }}{% endif %}
Using chat eos_token: <|end_of_text|>
Using chat bos_token: <|begin_of_text|>
prompt: >? 経歴は?
llama_perf_context_print:        load time =   10177.09 ms
llama_perf_context_print: prompt eval time =   10176.60 ms /    10 tokens ( 1017.66 ms per token,     0.98 tokens per second)
llama_perf_context_print:        eval time =  215376.77 ms /    16 runs   (13461.05 ms per token,     0.07 tokens per second)
llama_perf_context_print:       total time =  225579.91 ms /    26 tokens
AI: postgresql 3年 / Vuejs 3年 / Java 半年です

```
